---
title: "Python Class Project"
output: html_document
editor_options: 
  chunk_output_type: console
---

Load data:

```{r}
library(stm)
library(tm)
library(quanteda)
library(caret)

setwd("C:/Users/Matt/OneDrive - The Ohio State University/Summer 20/Python Class/Project")

data <- read.csv("inequalitydocuments_handcode.csv")

data$realid <- as.character(data$realid)

data$lab <- as.factor(data$lab)

data$treatment <- as.factor(data$treatment)

data$response <- as.character(data$response)

data$ideology_ra <- as.factor(data$ideology_ra)

data$left <- as.factor(data$left)

data$extreme <- as.factor(data$extreme)

str(data)
```

Preprocess:

```{r}
dfm <- dfm(data$response, tolower=TRUE, stem=TRUE, remove=stopwords("english"),
           remove_punct=TRUE)

dfm <- convert(dfm, to="data.frame")

dfm$numberid_meta <- data$numberid

dfm$realid_meta <- data$realid

dfm$treatment_meta <- data$treatment

dfm$lab_meta <- data$lab

dfm$ideology_ra_meta <- data$ideology_ra

dfm$left_meta <- data$left

dfm$extreme_meta <- data$extreme

dfm <- dfm[,-1]

dfm <- dfm[,c(1735:1741, 1:1734)]

```

Make test and training set:

```{r}
#subset out mturk data

dfm_train_mturk <- subset(dfm, lab_meta==0)

#subset out lab data

lab_subset <- subset(dfm, lab_meta==1)

#subset out treatment condition

treatment_subset <- subset(lab_subset, treatment_meta==1)

#subset out control condition

control_subset <- subset(lab_subset, treatment_meta==0)

#no need to keep lab subset or complete dfm

rm(lab_subset, dfm)

#select which observations to sample from treatment and control 
#subsets so that there are equal numbers of each

sampletreatment <- sample(1:117, size=65, replace=FALSE)

samplecontrol <- sample(1:117, size=65, replace=FALSE)

#make these into training sets

treatment_train <- treatment_subset[sampletreatment,]

control_train <- control_subset[samplecontrol,]

#make the whole training set (mturk + treatment_train + control_train)

dfm_train <- do.call("rbind", list(dfm_train_mturk, treatment_train, control_train))

#remove unnecessary dataframes

rm(control_train, dfm_train_mturk, treatment_train)

#make test set out of remaining lab data

treatment_test <- treatment_subset[-sampletreatment,]

control_test <- control_subset[-samplecontrol,]

dfm_test <- rbind(treatment_test, control_test)

#remove unnecessary objects

rm(control_subset, control_test, treatment_subset, treatment_test, samplecontrol,
   sampletreatment)

```

Start running models:

```{r}

# dfm_train <- dfm_train[,-c(1:4)]
# 
# dfm_train_complete <- dfm_train[complete.cases(dfm_train)==TRUE,]
# 
# mod_enet <- train(ideology_meta~., method="glmnet",
# tuneGrid=expand.grid(alpha=seq(0, 1, 0.1), 
# lambda=seq(0,100, 10)),
# data=dfm_train_complete,
# preProcess=c("center"),
# trControl=trainControl(method="cv",number=2, search="grid"))
# 
# mod_enet$bestTune$alpha
# 
# mod_enet$bestTune$lambda
# 
# mod_enet_2 <- train(ideology_meta~., method="glmnet",
# tuneGrid=expand.grid(alpha=seq(0, 0.2, 0.05), 
# lambda=seq(0,20, 1)),
# data=dfm_train_complete,
# preProcess=c("center"),
# trControl=trainControl(method="cv",number=2, search="grid"))
# 
# mod_enet_2$bestTune$alpha
# 
# mod_enet_2$bestTune$lambda
# 
# mod_enet_3 <- train(ideology_meta~., method="glmnet",
# tuneGrid=expand.grid(alpha=seq(0, 0.15, 0.01), 
# lambda=seq(0,2,0.2)),
# data=dfm_train_complete,
# preProcess=c("center"),
# trControl=trainControl(method="cv",number=2, search="grid"))
# 
# mod_enet_3$bestTune$alpha
# 
# mod_enet_3$bestTune$lambda
# 
# mod_enet_4 <- train(ideology_meta~., method="glmnet",
# tuneGrid=expand.grid(alpha=seq(0.1, 0.15, 0.01), 
# lambda=seq(0,1,0.01)),
# data=dfm_train_complete,
# preProcess=c("center"),
# trControl=trainControl(method="cv",number=2, search="grid"))
# 
# mod_enet_4$bestTune$alpha
# 
# mod_enet_4$bestTune$lambda
# 
# mod_enet_5 <- train(ideology_meta~., method="glmnet",
# tuneGrid=expand.grid(alpha=seq(0, 0.15, 0.01), 
# lambda=seq(0,0.5,0.01)),
# data=dfm_train_complete,
# preProcess=c("center"),
# trControl=trainControl(method="cv",number=2, search="grid"))
# 
# mod_enet_5$bestTune$alpha
# 
# mod_enet_5$bestTune$lambda
# 
# rm(mod_enet, mod_enet_2, mod_enet_3, mod_enet_4)
# 
# dfm_train_complete$yhat <- predict(mod_enet_5, dfm_train_complete)
# 
# summary(dfm_train_complete$ideology_meta)
# summary(dfm_train_complete$yhat)
# 
# dfm_train_complete$false <- ifelse(dfm_train_complete$ideology_meta==dfm_train_complete$yhat, 0, 1)
# 
# summary(dfm_train_complete$false)
# 
# table(dfm_train_complete$false)
# 
# 1-mean(dfm_train_complete$false)
# 
# #So about 95% accuracy
# 
# #look at test set
# 
# dfm_test <- dfm_test[,-c(1:4)]
# 
# dfm_test_complete <- dfm_test[complete.cases(dfm_test)==TRUE,]
# 
# dfm_test_complete$yhat <- predict(mod_enet_5, dfm_test_complete)
# 
# summary(dfm_test_complete$ideology_meta)
# summary(dfm_test_complete$yhat)
# 
# dfm_test_complete$false <- ifelse(dfm_test_complete$ideology_meta==dfm_test_complete$yhat, 0, 1)
# 
# summary(dfm_test_complete$false)
# 
# table(dfm_test_complete$false)
# 
# 1-mean(dfm_test_complete$false)
# 
# #63% accuracy
# 
# mean(as.numeric(dfm_test_complete$ideology_meta)-1)

#Not really any better than guessing

#Could upsample some 0s and so forth

########################################################################
########################################################################
########################################################################

# Random forest -- make sure to remove yhats added to training set

# randomforest <- train(ideology_meta~., method="rf",
# data=dfm_train_complete)
# 
# dfm_train_complete$yhat <- predict(randomforest, dfm_train_complete)
# 
# summary(dfm_train_complete$ideology_meta)
# summary(dfm_train_complete$yhat)
# 
# dfm_train_complete$false <- ifelse(dfm_train_complete$ideology_meta==dfm_train_complete$yhat, 0, 1)
# 
# summary(dfm_train_complete$false)
# 
# table(dfm_train_complete$false)
# 
# 1-mean(dfm_train_complete$false)

#So about 95% accuracy

#look at test set

# dfm_test_complete$yhat <- predict(randomforest, dfm_test_complete)
# 
# summary(dfm_test_complete$ideology_meta)
# summary(dfm_test_complete$yhat)
# 
# dfm_test_complete$false <- ifelse(dfm_test_complete$ideology_meta==dfm_test_complete$yhat, 0, 1)
# 
# summary(dfm_test_complete$false)
# 
# table(dfm_test_complete$false)
# 
# 1-mean(dfm_test_complete$false)

#63% accuracy

# mean(as.numeric(dfm_test_complete$ideology_meta)-1)

#66% to %59 -- sort of better

```



